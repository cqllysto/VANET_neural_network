# -*- coding: utf-8 -*-
"""Working_Neural_Network_Research.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qn8KD2H4UQTDc_BsE3mtVWqicOvExTYd
"""

!pip install bayesian-optimization

import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from bayes_opt import BayesianOptimization, UtilityFunction
import warnings
warnings.filterwarnings("ignore")

import math
import numpy as np
from scipy import special
import pandas as pd
import time

from keras import backend as K
from keras.models import Sequential, Model
from keras import layers, regularizers
from keras import Input
from sklearn.preprocessing import MinMaxScaler
from keras.layers import Lambda

# be sure to change the file path
# if you have the dataset in another
# directly than the working folder
# /content/final_data.csv
#/content/data_stepsize_0_point_1.csv
df = pd.read_csv('/content/data_stepsize_0_point_1.csv')
#df = df.drop('Index',axis=1)
df.head()

# Cleaning the Data

# 75% of the data is selected
train_df = df.sample(frac=0.75, random_state=4)

# it drops the training data
# from the original dataframe
val_df = df.drop(train_df.index)

# calling to (0,1) range
max_val = df.max(axis= 0)
min_val = df.min(axis= 0)


range = max_val - min_val
train_df = (train_df - min_val)/(range)

val_df =  (val_df- min_val)/range


# now let's separate the targets and labels
X_train = train_df.drop(['CBR','PRP','ed'], axis=1)

X_val = val_df.drop(['CBR','PRP','ed'], axis=1)

y_train = train_df[['CBR']].copy()

y_val = val_df[['CBR']].copy()

# We'll need to pass the shape
# of features/inputs as an argument
# in our model, so let's define a variable
# to save it.
input_shape = [X_train.shape[1]] # How many inputs we have

# print(input_shape, min_val, max_val, range)
print(input_shape)

#############################################
#       NEURAL NETWORK MODEL
#############################################

model = tf.keras.Sequential([
    tf.keras.layers.GaussianNoise(0.001, seed=None),
    tf.keras.layers.Dense(units=64,
                          activation='relu',
                          kernel_regularizer=tf.keras.regularizers.L2(0.001),
                          input_shape=input_shape),
    tf.keras.layers.Dense(units=128, activation='relu'),
    tf.keras.layers.Dense(units=32, activation='relu'),
    tf.keras.layers.Dense(units=1)
])



# adam optimizer works pretty well for
# all kinds of problems and is a good starting point
model.compile(optimizer='adam',

              # MAE error is good for
              # numerical predictions
              loss='mae')

losses = model.fit(X_train, y_train,

                   validation_data=(X_val, y_val),

                   # it will use 'batch_size' number
                   # of examples per example
                   batch_size=64,
                   epochs=50,  # total epoch

                   )

# 1

# Visualizing training Vs Validation Loss

loss_df = pd.DataFrame(losses.history)

# history stores the loss/val
# loss in each epoch

# loss_df is a dataframe which
# contains the losses so we can
# plot it to visualize our model training
loss_df.loc[:,['loss','val_loss']].plot()

# y_val.shape
# y_val.iloc[0:5,:]
y_val = pd.DataFrame(y_val)
y_val = y_val.values.tolist()
results = model.predict(X_val.iloc[:,:])
results = pd.DataFrame(results)
results = results.values.tolist()
results = [i[0] for i in results]
y_val = pd.DataFrame(y_val).to_numpy()
results = pd.DataFrame(results).to_numpy()
values = np.stack((y_val[:,0], results[:,0].T), axis=1)
values.shape
pd.DataFrame(values).to_csv('values.csv')

list_array = list(np.arange(0,len(y_val)))
sum:float = 0.0
dif:float = 0.0
for i in list_array:
  sum += abs((results[i]-y_val[i])/y_val[i])*100
  dif += abs(results[i]-y_val[i])
avg_error = sum/(len(list_array))
dif = dif/(len(list_array))
print(avg_error,dif)

import time
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from bayes_opt import BayesianOptimization, UtilityFunction
import warnings
warnings.filterwarnings("ignore")

import math
from numpy import dot
from scipy import special

def qosGenerator(gammap, k, lambdap, Nrp, W0 = 1024, beita = 0.2, Nf_value = 1.2589e-13):

    def SubrSNRC_pi(M=None, re=None, T=None, W0=None, lambdap=None, DIFS=None, sigma=None, *args, **kwargs):

        R = re
        lambda_ = lambdap / 1000000.0
        Ntr = M
        Ncs = Ntr
        Nph = dot(2.0, Ntr) - Ncs
        rho = 1
        rho_old = 0
        pb = 1
        epsilon = 1e-06
        while (abs(rho_old - rho) > epsilon):
            pix = dot(2, T) / (dot(W0, (sigma + dot(pb, T))) + sigma - dot(pb, T) + dot(2, T) + dot(dot(2, (1 - rho)),
                                                                                                    (1 / lambda_ + DIFS)))
            Pxmt = dot((T - DIFS - dot(2, sigma)), pix) / W0 / T + dot(dot(dot((1 - 1 / W0), 2), sigma), pix) / T
            pb1 = 1 - np.exp(dot(- Ntr, Pxmt))
            if (pb1 < 0):
                pb1 = 0
            while (abs(pb1 - pb) > epsilon):
                pix = dot(2, T) / (dot(W0, (sigma + dot(pb, T))) + sigma - dot(pb, T) + dot(2, T) + dot(dot(2, (1 - rho)), (
                        1 / lambda_ + DIFS)))
                Pxmt = dot((T - DIFS - dot(2, sigma)), pix) / W0 / T + dot(dot(dot((1 - 1 / W0), 2), sigma), pix) / T
                pbb = 1 - np.exp(dot(- Ntr, Pxmt))
                if (pbb < 0):
                    pbb = 0
                pb1 = pb
                pb = pbb
            pb = pb1
            qb = 1 - (1 - pb) ** (dot((T + DIFS), W0) / (T - DIFS + dot(dot(2, sigma), W0)))
            mu_tem = 2 / (dot(dot((sigma + dot(pb, T)), (1 - dot((1 - qb), (1 - rho)))), (W0 - 1)) + dot(2, T))
            rho_old = rho
            if (lambda_ < mu_tem):
                rho = lambda_ / mu_tem
            else:
                rho = 1
        qb = 1 - (1 - pb) ** (dot((T + DIFS), W0) / (T - DIFS + dot(dot(2, sigma), W0)))
        mu = 2 / (dot(dot((sigma + dot(pb, T)), (1 - dot((1 - qb), (1 - rho)))), (W0 - 1)) + dot(2, T))
        # ==========================================================================
        var_PA = 0
        mean_service = 1 / mu / 1000000.0
        var_service = dot((dot(dot(dot((W0 - 1), (dot(2, W0) - 1)), (sigma + dot(pb, T)) ** 2),
                            (1 - dot((1 - qb), (1 - rho)))) / 6 + dot(
            dot((W0 - 1), (dot(var_PA, pb) + dot(dot(T ** 2, pb), (1 - pb)) + dot(dot(2, T), (sigma + dot(pb, T))))),
            (1 - dot((1 - qb), (1 - rho)))) / 2 + var_PA + T ** 2 - (
                                dot(dot((sigma + dot(pb, T)), (1 - dot((1 - qb), (1 - rho)))),
                                    (W0 - 1)) / 2 + T) ** 2), (1 / 1000000.0) ** 2)
        # ==========================================================================
        EDq = dot(lambdap, (var_service + mean_service ** 2)) / (dot(2, (1 - rho)))
        ED = EDq + mean_service
        # ==========================================================================
        pi_XMT = dot(2, T) / (
                dot((rho + dot(qb, (1 - rho))), (dot((sigma + dot(pb, T)), W0) + (sigma - dot(pb, T)))) + dot(2,
                                                                                                            T) + dot(
            dot(2, (1 - rho)), (1 / lambda_ + DIFS)))
        P_XMT = dot(pi_XMT, (T - DIFS + dot(dot(2, sigma), W0))) / (dot(W0, T))
        # --------------------------------------------------------------------------
        pi_0 = dot(dot(pi_XMT, (rho + dot(qb, (1 - rho)))), sigma) / T
        # ==========================================================================
        pt = dot(dot(pi_XMT, 2), (T - DIFS)) / T
        return pt, pi_0, pi_XMT, ED


    if __name__ == '__main__':
        pass


    #initialization
    ds = 100      #change from 350
    R = 100
    #beita = 0.2
    M = 1
    gammap=gammap ##########NEW#ADD###############

    Band = 20
    PAn = 1600
    nds = 52
    cr = 5 / 6
    nhmac = 64
    tbdh = 4
    kb = Band / 5
    sigma = 21 / kb
    SIFS = 64 / kb
    AIFS = SIFS + dot(2, sigma)
    tsym = 16 / kb
    tpre = math.floor(160 / kb)
    nmimo = 1 ##### MIMO rate = 2在原来的代码里

    #Payload duration calculation
    Ts = tpre + AIFS + dot(tsym, np.ceil((dot(PAn, 8) + nhmac) / (dot(dot(k, nds), cr)))) + tbdh
    Ts = Ts / nmimo
    Tp = (Ts - tpre - AIFS - tbdh)
    T = Ts + dot(Tp, (Nrp))

    #Network parameter settingup
    d0 = 1
    dc = 102
    Pt = 0.28183815
    Rth = 1.3969e-15
    Pth = 1.3969e-15
    Inta = 1.63726e-05
    Nf = Nf_value          #dot(0.0001, 1.2589e-13)  ################NEW#ADD##################
    alfa1 = 2.56
    alfa2 = 6.34
    rE = dot((gammap) ** (1 / alfa2), R)
    sigma1 = 3.9
    sigma2 = 5.2

    M = round(dot(dot(2, rE), beita)) ################NEW#ADD##################
    #The Density of vehicles on the road
    #beta(M)=beita   #beta(M)=M/R/2 旧代码

    #Call the subroutine to derive pt, pi_0, and pi_{XMT} from Eq. (7)
    pt, pi_0, pi_XMT, ED = SubrSNRC_pi(M, rE, T, W0, lambdap, AIFS, sigma)
    #Calculate QoS metrics
    #256QAM with 8 dB Gain
    ed = ED
    pt = dot(pt, (1 + 1 / (Nrp + 1))) / 2
    B1 = Band ####################原来是B1=20
    Dr = 68.82
    Sr = np.arange(0, 1300, 0.5)
    EbNo = dot(dot(Sr, 10 ** 0.8), B1) / Dr
    MM = 2 ** k
    x3 = np.sqrt(dot(dot(3, k), EbNo) / (MM - 1))
    Pbg = dot(dot(dot((4 / k), (1 - 1 / np.sqrt(MM))), (1 / 2)), special.erfc(x3 / math.sqrt(2)))
    FER = 1 - (1 - Pbg) ** (dot(PAn, 8))

    if ds <= dc:
        sigma = sigma1
    else:
        sigma = sigma2
    theta = 0

    CSINR=np.ones(2600)
    dt=np.ones(2600)
    while theta < 1300:
        theta1 = int(1 + np.floor(dot(theta, 2))) - 1
        if ds <= dc:
            DImax = dot((theta) ** (1 / alfa1), ds)
            if DImax > dc:
                DImax = dot(dot((theta) ** (1 / alfa2), (ds / dc) ** (alfa1 / alfa2)), dc)
        else:
            DImax = dot(dot((theta) ** (1 / alfa1), (ds / dc) ** (alfa2 / alfa1)), dc)
            if DImax > dc:
                DImax = dot((theta) ** (1 / alfa2), ds)
        if ds <= dc:
            DImax2 = dot((dot(2, theta)) ** (1 / alfa1), ds)
            if DImax2 > dc:
                DImax2 = dot(dot((dot(2, theta)) ** (1 / alfa2), (ds / dc) ** (alfa1 / alfa2)), dc)
        else:
            DImax2 = dot(dot((dot(2, theta)) ** (1 / alfa1), (ds / dc) ** (alfa2 / alfa1)), dc)
            if DImax2 > dc:
                DImax2 = dot((dot(2, theta)) ** (1 / alfa2), ds)
        N_ht = max(DImax - rE + ds, 0) + max(DImax - rE - ds, 0)
        Psh = 1 - np.exp(dot(dot(- pt, N_ht), beita))
        N_ht21 = (max(DImax2 - max(DImax, rE + ds), 0))
        N_ht22 = (max(DImax2 - max(DImax, rE - ds), 0))
        Phc = dot((1 - np.exp(dot(dot(- pt, N_ht21), beita))), (1 - np.exp(dot(dot(- pt, N_ht22), beita))))
        Ncc = min(DImax, rE - ds) + min(DImax, rE + ds)
        Pcc = 1 - np.exp(dot(dot(- pi_0, Ncc), beita))
        Pt1=gammap*Pt ################NEW#ADD##################

        if ds <= dc:
            Pr = dot(dot(Pt1, Inta), (d0 / ds) ** alfa1)################NEW#ADD##################change Pt to Pt1
        else:
            Pr = dot(dot(dot(Pt1, Inta), (d0 / dc) ** alfa1), (dc / ds) ** alfa2)################NEW#ADD##################change Pt to Pt1
        Prdb = dot(10, np.log10(Pr))
        Fn = dot(1 / 2, (1 - special.erf((Prdb - dot(10, np.log10(dot(Nf, theta)))) / 2 ** (1 / 2) / sigma)))

        Prth = dot(1 / 2, (1 - special.erf((Prdb - dot(10, np.log10(Rth))) / 2 ** (1 / 2) / sigma)))
        CSINR[theta1] = 1 - dot(dot(dot((1 - Psh), (1 - Phc)), (1 - Pcc)), (1 - Fn))
        dt[theta1] = theta
        theta = theta + 0.5

    #Calculate the dFSINR
    dFSINR = np.diff(CSINR) / np.diff(dt)

    #Calculate the integral
    sum = 0
    for th in range(1, 2598 + 1):   # Might be arange(1, 2598+1, 1)
        num = dot(dot(FER[th - 1], dFSINR[th - 1]), 0.5)
        sum = sum + num


    Integration = sum
    tPRP = dot((1 - Integration), (1 - Prth))

    #Transmission capacity and throughput
    Pdc = 1 - np.exp(dot(dot(dot(- pi_0, 2), beita), rE))################NEW#ADD##################change R to rE
    Pdh = (1 - np.exp(dot(dot(- pt, rE), beita) / 2)) ** 2################NEW#ADD##################change R to rE
    TC = dot(dot(dot(dot(2, lambdap), rE), beita), (1 - Pdc / 2 - Pdh / 2))################NEW#ADD##################change R to rE
    CBR = dot(dot(TC, T), 1e-06)
    cbr = CBR
    Thruput = dot(CBR, (Nrp + 1))

    #PRP with repetitions
    ss = 0
    for i in range (1, int(Nrp) + 1 + 1):  #(1, Nrp.astype(np.int64) + 1 + 1):
        ss = ss + dot(dot(special.comb(Nrp + 1, i), tPRP ** i), (1 - tPRP) ** (Nrp + 1 - i))
    PRP = ss


    #print('prp:%f\n'% PRP)
    #print('ed:%f\n'% ed)
    #print('cbr:%f'% CBR)



    #########################
    # ---   constrain   --- #
    #########################


    return -CBR

def neural_network(gammap, k, lambdap, Nrp):
    # Reshape the inputs into the format expected by the neural network
    inputs = np.array([[gammap, k, lambdap, Nrp]])
    max_val = inputs.max(axis=1)
    min_val = inputs.min(axis=1)

    range = max_val - min_val
    inputs = (inputs - min_val)/(range)

    # Pass the inputs through the neural network to get the output
    output = model.predict(inputs)
    #output = 0.016649841 + output*1.138632308

    if output[0][0] <= 0:
      return qosGenerator(gammap, k, lambdap, Nrp)

      # Force to meet QOS requirements

    else:
      return -output[0][0]
      # Return the output as a scalar value
      #if PRP_or_ED(gammap, k, lambdap, Nrp) == 1:
        #return -output[0][0]
      #else:
        #return -1000

# Optimizing the surrogate function
# Create the optimizer. The black box function to optimize is not
# specified here, as we will call that function directly later

t1 = time.perf_counter()
optimizer = BayesianOptimization(f = neural_network,
                                 pbounds = {'gammap': [1,10], 'k': [6,10], 'lambdap': [1,2], 'Nrp': [1,2]},
                                 verbose = 2, random_state = 1234,
                                 allow_duplicate_points=True)

# let the init_points = 5
# let the iteration = 100

a = optimizer.maximize(init_points = 5, n_iter = 100)
t2 = time.perf_counter()
print("Best result: {}; f(x) = {}.".format(optimizer.max["params"], -1 * optimizer.max["target"]),'time taken to run:',t2-t1)



# qosGenerator(gammap, k, lambdap, Nrp)
qosGenerator(6.599, 7.751, 1.0,1.0)

# Here the optimum values are obtained by calculating the cost of a
# particular position in the loop
# Each variable stands for a different input variable
# and the constraints are the QoS requirments that need to be met
# Inputs is the variable in which the optimum inputs will stored
lowcost = float('inf')  # Set initial lowcost to infinity

#var = inputs[0]
#var = inputs[1]
#var = inputs[2]
#var = inputs[3]
#outputs = qosGenerator(inputs[0], inputs[1], inputs[2], inputs[3])
#constraint = outputs[0]

inputs = None
var1 = 60
var2 = 10
var3 = 1
var4 = 1
constraint2 = 0.999
constraint1 = 0.0124198

# The function below calculates the cost at a particular iteration
# The first constraint costs more and the second cost less
# The differing costs ensure the QoS requirements which need to be the lowest
# will be the lowest
def cost(i, j, k, l):
    try:
        outputs = qosGenerator(i, j, k, l)
        middlecost = 0
        totalcost = 0
        if outputs[1] >= 1:
            middlecost = 1000
        elif outputs[1] < 0.999:
            middlecost = 1000
        else:
            middlecost = (outputs[1] - constraint2) * 10
        totalcost = (abs(abs(outputs[0]) - constraint1)) * 1000 + middlecost
        return totalcost
    except ZeroDivisionError:
        return float('inf')  # Return a large value in case of ZeroDivisionError

# This is an ordinary loop to iterate through the values of the first input variable
# Cost is calculated and if the calculated cost is the lowest one till then,
# The cost is stored in lowcost and the inputs are stores in inputs variable
range1 = abs(var1 - 0)
range2 = abs(10 - var1)

for i in range(var1 - 10, var1 + 10):
    i = i / 10
    cost1 = cost(i, 10, 1, 1)
    if cost1 < lowcost:
        inputs = (i, 10, 1, 1)
        lowcost = cost1
    lowcost = secondloop(i, lowcost)

#This function calculates the lowest cost among all other iterations
# and returs it
def secondloop(i, lowcost):
    for j in range(var2 -2, var2 + 2, 2):
        cost2 = cost(i, j, 1, 1)  # Modified the cost function call
        if cost2 < lowcost:
            lowcost = cost2
            inputs = (i, j, 1, 1)
            for l in range(var3 - 2, var3 - 2):
                cost3 = cost(i, j, l, 1)
                if cost3 < lowcost:
                    lowcost = cost3
                    inputs = (i, j, l, 1)
                    for m in range(var4 -2, var4 -2):
                        cost4 = cost(i, j, l, m)
                        if cost4 < lowcost:
                            lowcost = cost4
                            inputs = (i, j, l, m)
    return lowcost

print(inputs)
# Sometimes the obtained values will not full fulfill QoS requirements
# This loop will make sure only the input values that does are printed
outputs = qosGenerator(inputs[0], inputs[1], inputs[2], inputs[3])
if(outputs[1] >= 0.999) & (outputs[2] <= 0.01):
    print(inputs)
else:
    print("Error in obtaining optimized values, increase range for input variables")

"""NOTES ON REGULARIZATION:
* Speed of training significantly improves when Gaussian noise is added.
* When Dropout layer is added, relative error (RE) increased to 157% and absolute error (AE) increase to 0.13
* When tanh is used, RE = 35% and AE = 0.01

"Parameters Used and Error Table" \
Features: Gaussian Noise, Regularization of input layer, relu activation function \
Error: RE = 2.8%, AE = 0.003 \

Features: Gaussian Noise, no regularization, relu activation function \
Error: RE = 3.3%, AE = 0.0026 \

Features: Gaussian Noise, regularization on each layer, relu activation function \
Error: RE = 4.4%, AE = 0.0033 \

Features: No Gaussian Noise, regularization on first input layer, relu activation function \
Error: RE = 2.97%, AE = 0.00265 \

Change density from 0.2 to 0.3, Generate new data (clean final data) train the neural network again and see if the opitimization still works.

ALSO
Change megahertz to 120MHz from 20MHz and keep density at 0.2
"""